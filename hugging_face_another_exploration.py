# -*- coding: utf-8 -*-
"""hugging_face_another_exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EcGWvJzuiVvQ2o9hNWpwVlbdGhx89Yq5
"""

!pip uninstall transformers
!pip install transformers==4.31.0

import pandas as pd
import numpy as np
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
data = pd.read_csv('data_major_with_information.csv')
data['text'] = data['Description'] + ' ' + data['Mata Kuliah']

# Encode the labels
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['NAMA'])

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(text):
    return tokenizer(text, padding='max_length', truncation=True, max_length=128)

# tokenized_data = data['text'].apply(lambda x: tokenize_function(x))
tokenized_data = data['information'].apply(lambda x: tokenize_function(x))
input_ids = np.array([x['input_ids'] for x in tokenized_data])
attention_masks = np.array([x['attention_mask'] for x in tokenized_data])

def create_model():
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')
    input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')
    attention_masks = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_masks')

    bert_output = bert_model(input_ids, attention_mask=attention_masks)[1]
    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)
    dropout = tf.keras.layers.Dropout(0.2)(dense)
    output = tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')(dropout)

    model = tf.keras.models.Model(inputs=[input_ids, attention_masks], outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

# train_ids, val_ids, train_masks, val_masks, train_labels, val_labels = train_test_split(
#     input_ids, attention_masks, data['label'], test_size=0.2, random_state=42)

model = create_model()
model.summary()

model.fit([input_ids, attention_masks], data['label'], epochs=80, batch_size=16)

def find_top_10_similar_majors(input_text):
    tokenized_input = tokenize_function(input_text)
    input_ids = np.array([tokenized_input['input_ids']])
    attention_mask = np.array([tokenized_input['attention_mask']])

    predictions = model.predict([input_ids, attention_mask])
    top_10_indices = np.argsort(predictions[0])[::-1][:10]
    top_10_majors = label_encoder.inverse_transform(top_10_indices)

    return top_10_majors

# Example usage
input_text = "matematika"
top_10_majors = find_top_10_similar_majors(input_text)
print("Top-10 jurusan yang mungkin cocok: ", top_10_majors)

tokenizer.save_pretrained('saved_tokenizer')

# Save the model in PB format
model.save('saved_model', save_format='tf')

import shutil
import os

shutil.make_archive('model_and_tokenizer', 'zip', 'saved_model')


shutil.make_archive('saved_tokenizer', 'zip', 'saved_tokenizer')
shutil.move('saved_tokenizer.zip', 'model_and_tokenizer.zip')

!zip -r models.zip saved_model

!zip -r tokenizers.zip saved_tokenizer

!du -sh /content/saved_model

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/My\ Drive/

!cp models.zip /content/drive/My\ Drive/

!ls /content/drive/My\ Drive/

