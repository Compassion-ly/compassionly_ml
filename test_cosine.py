# -*- coding: utf-8 -*-
"""test_cosine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MazjD8Z1H1lOquK2Dw9a0WR7-rDpe0BV
"""


!pip uninstall transformers
!pip install transformers==4.31.0

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import tensorflow as tf
from transformers import TFDistilBertModel, DistilBertTokenizer
from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling1D
from tensorflow.keras.models import Model
from sklearn.metrics.pairwise import cosine_similarity

train_data = pd.read_csv('data_major_with_information.csv')
validation_data = pd.read_csv('major_data_validation.csv')

train_data

validation_data

train_data['text'] = train_data['information']
validation_data['text'] = validation_data['Deskripsi']


label_encoder = LabelEncoder()
train_data['label'] = label_encoder.fit_transform(train_data['highest'])
validation_data['label'] = label_encoder.transform(validation_data['highest'])

train_data['highest']

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
base_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')

def tokenize_texts(texts, tokenizer, max_len=128):
    return tokenizer(
        texts.tolist(),
        max_length=max_len,
        truncation=True,
        padding=True,
        return_tensors='tf'
    )

train_tokens = tokenize_texts(train_data['text'], tokenizer)
validation_tokens = tokenize_texts(validation_data['text'], tokenizer)
num_unique_classes = train_data['highest'].nunique()

from tensorflow.keras.layers import Input, GlobalAveragePooling1D, Dense, LayerNormalization, Dropout

input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')
base_model_output = base_model(input_ids, attention_mask=attention_mask)
pooled_output = GlobalAveragePooling1D()(base_model_output.last_hidden_state)
normalized_output = LayerNormalization()(pooled_output)
dropout_output = Dropout(0.4)(normalized_output)
output = Dense(len(label_encoder.classes_), activation='softmax',)(dropout_output)
# output = Dense((num_unique_classes), activation='softmax')(pooled_output)

model = Model(inputs=[input_ids, attention_mask], outputs=output)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

tf.config.run_functions_eagerly(True)

history = model.fit(
    [train_tokens['input_ids'], train_tokens['attention_mask']],
    train_data['label'],
    validation_data=([validation_tokens['input_ids'], validation_tokens['attention_mask']], validation_data['label']),
    epochs=100,
    batch_size=32
)

def tokenize_texts(texts, tokenizer, max_len=128):
    return tokenizer(
        texts,  # Directly use the list of texts
        max_length=max_len,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )

def get_embeddings(texts, tokenizer, model, max_len= 128):
    tokens = tokenize_texts(texts, tokenizer, max_len)
    embeddings = model.predict([tokens['input_ids'], tokens['attention_mask']])
    return embeddings

embeddings = get_embeddings(train_data['text'].tolist(), tokenizer, model)

input_text = "menyukai ilmu tentang bumi dan lingkungan serta teknologi yang dapat digunakan untuk menjaga dan mengelola sumber daya alam. Jurusan ini mempelajari berbagai fenomena alam seperti gempa bumi, gunung berapi, dan iklim serta cara untuk memprediksi, mencegah, dan mengurangi dampak negatifnya"
input_embedding = get_embeddings([input_text], tokenizer, model)[0]

similarities = cosine_similarity([input_embedding], embeddings)[0]
# top_10_indices = similarities.argsort()[-10:][::-1]
top_10_indices = similarities.argsort()[-100:][::-1]

top_10_majors = train_data.iloc[top_10_indices]['highest']
top_5_unique = top_10_majors.unique()[:5]
columns_to_consider = ['Matematika', 'Sains', 'Fisika', 'Sosiologi', 'Biologi', 'Kimia',
                       'Bisnis dan Ekonomi', 'Teknologi', 'Seni', 'Sastra dan Linguistik',
                       'Pendidikan', 'Hukum', 'Lingkungan', 'Kesehatan', 'Geografi',
                       'Komunikasi', 'Sejarah dan Filsafat']

num_to_major = {i+1: columns_to_consider[i] for i in range(len(columns_to_consider))}

# Map the top 5 unique numbers to their corresponding names
top_5_majors_names = [num_to_major[num] for num in top_5_unique]
print("Top-10 recommended majors:")
print(top_5_majors_names)

